{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b4ccfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6d1bd26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19909 total 27 unique\n"
     ]
    }
   ],
   "source": [
    "data = open('dino.txt', 'r').read()\n",
    "data= data.lower()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print('%d total %d unique' % (data_size, vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f58d3eba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\\n', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
     ]
    }
   ],
   "source": [
    "chars = sorted(chars)\n",
    "print(chars)\n",
    "\n",
    "#TODO: Add another charachter, the empty charachter that just follows newlines\n",
    "#Instead of having empty strings there that are guarantee to increase the loss function\n",
    "\n",
    "char_to_ix = { ch:i for i,ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16a26604",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataList = data.split()\n",
    "np.random.shuffle(dataList)\n",
    "\n",
    "mlen = 0\n",
    "for dino in dataList:\n",
    "    if len(dino) > mlen:\n",
    "        mlen = len(dino)\n",
    "\n",
    "n_x = n_y = len(chars)\n",
    "m = len(dataList)\n",
    "T_x = T_y = mlen + 1\n",
    "X = np.zeros((n_x, m, T_x), dtype = np.float32)\n",
    "Y = np.zeros((n_y, m, T_y), dtype = np.float32)\n",
    "\n",
    "for ex in range(len(dataList)):\n",
    "    for tx in range(len(dataList[ex])):\n",
    "        ch = char_to_ix[dataList[ex][tx]]\n",
    "        #X[0, :, :] = None, since we want it to generate dino names from scratch\n",
    "        X[ch, ex, tx + 1] = 1\n",
    "        Y[ch, ex, tx] = 1\n",
    "    for i in range(tx + 1, 27):\n",
    "        if i != 26:\n",
    "            X[char_to_ix['\\n'], ex, i + 1] = 1\n",
    "        Y[char_to_ix['\\n'], ex, i] = 1\n",
    "\n",
    "#Set final value of y to \\n to represent EOS token\n",
    "# Y[char_to_ix['\\n'], :, T_y - 1] += 1\n",
    "        \n",
    "X = tf.constant(X, dtype = tf.float32)\n",
    "Y = tf.constant(Y, dtype = tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e251d933",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip(gradients, maxValue):\n",
    "    \n",
    "    dWaa, dWax, dWya, dba, dby = gradients['dWaa'], gradients['dWax'], gradients['dWya'], gradients['dba'], gradients['dby']\n",
    "   \n",
    "    for gradient in [dWax, dWaa, dWya, dba, dby]:\n",
    "        gradient = tf.clip_by_value(gradient, -1 * maxValue, maxValue)\n",
    "        \n",
    "    gradients = {\"dWaa\": dWaa, \"dWax\": dWax, \"dWya\": dWya, \"dba\": dba, \"dby\": dby}\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "def sample(parameters, char_to_ix):\n",
    "    \n",
    "    Waa, Wax, Wya, by, ba = parameters['Waa'], parameters['Wax'], parameters['Wya'], parameters['by'], parameters['ba']\n",
    "    vocab_size = by.shape[0]\n",
    "    n_a = Waa.shape[1]\n",
    "    \n",
    "    x = tf.zeros([vocab_size, 1], dtype = tf.float32)\n",
    "    a_prev = tf.zeros([n_a, 1], dtype = tf.float32)\n",
    "    \n",
    "    indices = []\n",
    "    \n",
    "    idx = -1\n",
    "    \n",
    "    counter = 0\n",
    "    newline_character = char_to_ix['\\n']\n",
    "    \n",
    "    while (idx != newline_character and counter != 50):\n",
    "        \n",
    "        a, y = rnn_single_cell(Waa, Wax, Wya, ba, by, a_prev, x)\n",
    "\n",
    "        idx = np.random.choice(range(y.shape[0]), p = y.numpy().ravel())\n",
    "        \n",
    "        indices.append(idx)\n",
    "        \n",
    "        x = np.zeros((vocab_size, 1), dtype = np.float32)\n",
    "        x[idx] = 1\n",
    "        x = tf.constant(x, dtype = tf.float32)\n",
    "        \n",
    "        a_prev = a\n",
    "        \n",
    "        counter +=1\n",
    "\n",
    "    if (counter == 50):\n",
    "        indices.append(char_to_ix['\\n'])\n",
    "    \n",
    "    return indices\n",
    "\n",
    "def indices_to_str(indices, ix_to_char):\n",
    "    dinoname = ''\n",
    "    for ix in indices:\n",
    "        dinoname += ix_to_char[ix]\n",
    "        \n",
    "    return dinoname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f33c05d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_a, n_x, n_y):\n",
    "    \n",
    "    parameters = {\n",
    "        \"Waa\" : tf.Variable(tf.random.normal([n_a, n_a], dtype = tf.float32)),\n",
    "        \"Wax\" : tf.Variable(tf.random.normal([n_a, n_x], dtype = tf.float32)),\n",
    "        \"Wya\" : tf.Variable(tf.random.normal([n_y, n_a], dtype = tf.float32)),\n",
    "        \"ba\" : tf.Variable(tf.zeros([n_a, 1], dtype = tf.float32)),\n",
    "        \"by\" : tf.Variable(tf.zeros([n_y, 1], dtype = tf.float32))\n",
    "    }\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "@tf.function\n",
    "def rnn_single_cell(Waa, Wax, Wya, ba, by, a_prev, x):\n",
    "    a_next = tf.math.tanh(Waa @ a_prev + Wax @ x + ba)\n",
    "    y_hat = tf.nn.softmax(Wya @ a_next + by, axis = 0)\n",
    "    \n",
    "    return a_next, y_hat\n",
    "\n",
    "def optimize(X, Y, a_prev, parameters, optimizer, vocab_size = 27):\n",
    "    \n",
    "    Waa, Wax, Wya = parameters[\"Waa\"], parameters[\"Wax\"], parameters[\"Wya\"]\n",
    "    ba, by = parameters[\"ba\"], parameters[\"by\"]\n",
    "    \n",
    "    n_x, m, T_x = X.shape\n",
    "    n_y, m, T_y = Y.shape\n",
    "    \n",
    "    loss = tf.Variable(0, dtype = tf.float32)\n",
    "    \n",
    "    gradients = {}\n",
    "    \n",
    "    #Forward pass\n",
    "    with tf.GradientTape(watch_accessed_variables = False) as tape:\n",
    "        tape.watch(Waa)\n",
    "        tape.watch(Wax)\n",
    "        tape.watch(Wya)\n",
    "        tape.watch(ba)\n",
    "        tape.watch(by)\n",
    "        tape.watch(loss)\n",
    "        \n",
    "        #T_x = T_y so only need to do T_x times\n",
    "        for t in range(T_x):\n",
    "            \n",
    "            a_prev, y_hat = rnn_single_cell(Waa, Wax, Wya, ba, by, a_prev, X[:, :, t])\n",
    "            \n",
    "            #print(y_hat)\n",
    "            #print(Y[:, :, t])\n",
    "            loss = loss - (tf.multiply(Y[:, :, t], tf.math.log(y_hat)) + tf.multiply(1 - Y[:, :, t], tf.math.log(1 - y_hat + 0.0000001)))\n",
    "            #print(loss)\n",
    "                           \n",
    "        loss = tf.reduce_sum(loss / m, axis = [0, 1])\n",
    "    \n",
    "    \n",
    "    gradients = tape.gradient(loss, [Waa, Wax, Wya, ba, by])\n",
    "    for i in range(len(gradients)):\n",
    "        gradients[i] = tf.clip_by_value(gradients[i], -10, 10)\n",
    "    optimizer.apply_gradients(zip(gradients, [Waa, Wax, Wya, ba, by]))\n",
    "    \n",
    "\n",
    "    \n",
    "    return loss, parameters\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bef683c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([27, 1664, 27])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2bc41e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, \n",
    "          ix_to_char, \n",
    "          char_to_ix, \n",
    "          num_epochs = 35000, \n",
    "          n_a = 50, \n",
    "          learning_rate = 0.01, \n",
    "          minibatch_size = None, \n",
    "          num_dinos = 3, \n",
    "          parameters = None):\n",
    "    '''\n",
    "    Inputs:\n",
    "    X: Input data: tf.Constant of dimentions (n_x, m, T_x) \n",
    "    Y: Labels: tf.Constant of dimentions (n_y, m, T_y)\n",
    "    ix_to_char: Dictionary that converts numbers to their respective charachters\n",
    "    char_to_ix: Reverse of ix_to_char\n",
    "    n_a: Size of activation\n",
    "    minibatch_size: If set to None performs batch gradient descent\n",
    "    num_dinos: Number of dinos to sample every 2000 iterations\n",
    "    parameters: Default none, if continuing training from other params then put them in here\n",
    "    \n",
    "    Outputs: parameters - Dictionary containing weights required to sample\n",
    "    '''\n",
    "    \n",
    "    n_x, m, T_x = X.shape\n",
    "    \n",
    "    n_y, m, T_y = Y.shape\n",
    "    \n",
    "    if not parameters:\n",
    "        parameters = initialize_parameters(n_a, n_x, n_y)\n",
    "    \n",
    "    if m % minibatch_size != 0:\n",
    "        print(\"Minibatch size must divide m\")\n",
    "        return\n",
    "    \n",
    "    if not minibatch_size or minibatch_size > m:\n",
    "        minibatch_size = m\n",
    "    \n",
    "    num_mbatches = np.ceil(m / minibatch_size)\n",
    "    \n",
    "    last_loss = 0\n",
    "    \n",
    "    a0 = tf.Variable(np.zeros((n_a, minibatch_size)), dtype = tf.float32)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate)\n",
    "    \n",
    "    for j in range(num_epochs):\n",
    "        for mb in range(int(num_mbatches)):\n",
    "#             idx = j % (num_mbatches)\n",
    "        \n",
    "            start = int(mb * minibatch_size)\n",
    "            stop = int((mb + 1) * minibatch_size)\n",
    "        \n",
    "            if stop > m:\n",
    "                stop = m\n",
    "\n",
    "            Xt = X[:, start:stop, :]\n",
    "            Yt = Y[:, start:stop, :]\n",
    "\n",
    "            #a_prev = tf.clip_by_value(a_prev, 0, 0)\n",
    "\n",
    "            loss, parameters = optimize(Xt, Yt, a0, parameters, optimizer, vocab_size = T_x)\n",
    "        \n",
    "        \n",
    "        if j % 5 == 0:\n",
    "            print('Epoch: %d, Loss: %f' % (j, loss) + '\\n')\n",
    "            print(\"Seconds elapsed: {:.2f}\".format(time.time() - start_time))\n",
    "\n",
    "            print(\"Sampled Dino Names: \")\n",
    "            for dino in range(num_dinos):\n",
    "                    sampled_indices = sample(parameters, char_to_ix)\n",
    "                    dino = indices_to_str(sampled_indices, ix_to_char)\n",
    "                    print(dino)\n",
    "            \n",
    "            last_loss = loss\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9574c640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch size must divide m\n"
     ]
    }
   ],
   "source": [
    "params = model(X, Y, ix_to_char, char_to_ix, num_epochs = 3000, n_a = 50, minibatch_size = 256, num_dinos = 5, parameters = None, learning_rate = 0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ded8ff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "dinoParams = open(\"dinoParams.json\", \"wb\")\n",
    "\n",
    "pickle.dump(params, dinoParams)\n",
    "\n",
    "dinoParams.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "de1cddff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dinoParams.json', 'rb') as f:\n",
    "    params = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8d0547a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ossisaurus\n",
      "\n",
      "dachilysaurus\n",
      "\n",
      "alcatengsaurus\n",
      "\n",
      "delrasaurus\n",
      "\n",
      "braganodanodia\n",
      "\n",
      "dyihodsaurus\n",
      "\n",
      "tarisaurus\n",
      "\n",
      "girotevong\n",
      "\n",
      "sygostisaurus\n",
      "\n",
      "aurhanus\n",
      "\n",
      "saangosaurus\n",
      "\n",
      "pahrhenus\n",
      "\n",
      "centengosaurus\n",
      "\n",
      "asandmus\n",
      "\n",
      "tonklosaurra\n",
      "\n",
      "astrinstteraaa\n",
      "\n",
      "sinolrnathya\n",
      "\n",
      "neajisceps\n",
      "\n",
      "poreamrnloptops\n",
      "\n",
      "cartoceratops\n",
      "\n",
      "cinkyristes\n",
      "\n",
      "conklendosaurus\n",
      "\n",
      "colthalosaurus\n",
      "\n",
      "haptinus\n",
      "\n",
      "dytnosaurus\n",
      "\n",
      "heraavenator\n",
      "\n",
      "haplodrylongosaurus\n",
      "\n",
      "cogamosaurus\n",
      "\n",
      "prolur\n",
      "\n",
      "netokoflis\n",
      "\n",
      "deaaplansaurus\n",
      "\n",
      "sitonccanlurosaurus\n",
      "\n",
      "nechetoa\n",
      "\n",
      "ligen\n",
      "\n",
      "venighyn\n",
      "\n",
      "doshonnimys\n",
      "\n",
      "xitakrsaurus\n",
      "\n",
      "laaplovecerator\n",
      "\n",
      "goraatacosaurus\n",
      "\n",
      "camproniphalus\n",
      "\n",
      "liyrilsaurus\n",
      "\n",
      "eucnosaurus\n",
      "\n",
      "prorosaurus\n",
      "\n",
      "rulongus\n",
      "\n",
      "eorijtia\n",
      "\n",
      "caudosteus\n",
      "\n",
      "yevadasaurus\n",
      "\n",
      "khicdinsaurus\n",
      "\n",
      "roicosauruslogagsaurus\n",
      "\n",
      "ayrisaurus\n",
      "\n",
      "iptochatops\n",
      "\n",
      "hemilsus\n",
      "\n",
      "anctodonlar\n",
      "\n",
      "horusenatin\n",
      "\n",
      "nypfooplodrnthotinophossan\n",
      "\n",
      "matosaurus\n",
      "\n",
      "apelfihusaurus\n",
      "\n",
      "pronyuynol\n",
      "\n",
      "astanosaurus\n",
      "\n",
      "zhetanoplosus\n",
      "\n",
      "totosaurus\n",
      "\n",
      "vuephonasaurus\n",
      "\n",
      "unengapoa\n",
      "\n",
      "danataratos\n",
      "\n",
      "yurenkasaurus\n",
      "\n",
      "darporns\n",
      "\n",
      "juehansaurus\n",
      "\n",
      "tapatosaurus\n",
      "\n",
      "juioter\n",
      "\n",
      "stalovenathus\n",
      "\n",
      "siacsuin\n",
      "\n",
      "botanimosaurus\n",
      "\n",
      "baaethorathontitantajus\n",
      "\n",
      "sigisaurus\n",
      "\n",
      "bampodon\n",
      "\n",
      "zarngagisanaurus\n",
      "\n",
      "xiebholosaurus\n",
      "\n",
      "morocarmra\n",
      "\n",
      "suchongocronus\n",
      "\n",
      "rutomadon\n",
      "\n",
      "kersrgazax\n",
      "\n",
      "monys\n",
      "\n",
      "storenlis\n",
      "\n",
      "anmshosaurus\n",
      "\n",
      "nitingcorirator\n",
      "\n",
      "nonmuor\n",
      "\n",
      "yibhosaus\n",
      "\n",
      "salheosaurus\n",
      "\n",
      "yitosaurus\n",
      "\n",
      "acitosaus\n",
      "\n",
      "yiranisaurus\n",
      "\n",
      "priaraptor\n",
      "\n",
      "qitrrasaurus\n",
      "\n",
      "caeervosaurus\n",
      "\n",
      "astron\n",
      "\n",
      "harosaurus\n",
      "\n",
      "haplumaaurugugus\n",
      "\n",
      "aeatosaurus\n",
      "\n",
      "valuljosaurus\n",
      "\n",
      "culxovenator\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(100):\n",
    "    print(indices_to_str(sample(params, char_to_ix), ix_to_char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b28d9e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alnosaurus\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Waa, Wax, Wya, ba, by = params[\"Waa\"], params[\"Wax\"], params[\"Wya\"], params[\"ba\"], params[\"by\"]\n",
    "\n",
    "newline_char = char_to_ix['\\n']\n",
    "\n",
    "idx = -1\n",
    "\n",
    "a_prev = np.zeros((params[\"Waa\"].shape[0], 1), dtype = np.float32)\n",
    "x = np.zeros((params[\"Wax\"].shape[1], 1), dtype = np.float32)\n",
    "indices = []\n",
    "\n",
    "counter = 0\n",
    "    \n",
    "while (idx != newline_char):\n",
    "\n",
    "    a, y = rnn_single_cell(Waa, Wax, Wya, ba, by, a_prev, x)\n",
    "\n",
    "    idx = np.argmax(y.numpy(), axis = 0)[0]\n",
    "\n",
    "    indices.append(idx)\n",
    "\n",
    "    x = np.zeros((vocab_size, 1), dtype = np.float32)\n",
    "    x[idx] = 1\n",
    "    x = tf.constant(x, dtype = tf.float32)\n",
    "\n",
    "    a_prev = a\n",
    "\n",
    "    counter +=1\n",
    "\n",
    "print(indices_to_str(indices, ix_to_char))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
